{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1 PDFs, Total Documents: 13\n",
      "‚úÖ Split into 43 document chunks.\n"
     ]
    }
   ],
   "source": [
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "    from langchain_community.document_loaders import PyPDFLoader\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "    from langchain.chains import create_retrieval_chain\n",
    "    from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "    # Load environment variables (API keys, etc.)\n",
    "    load_dotenv()\n",
    "\n",
    "    #   Step 1: Load Multiple PDFs from a Folder\n",
    "    pdf_folder = \"pdfs\"  # Change this to your actual folder path\n",
    "\n",
    "    # Get all PDF files from the folder dynamically\n",
    "    pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "    # Load and merge all PDFs\n",
    "    docs = []\n",
    "    for pdf in pdf_files:\n",
    "        loader = PyPDFLoader(pdf)\n",
    "        docs.extend(loader.load())  # Append loaded content from each PDF\n",
    "\n",
    "    print(f\" Loaded {len(pdf_files)} PDFs, Total Documents: {len(docs)}\")\n",
    "\n",
    "    #   Step 2: Split text into smaller chunks for better processing\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "    print(f\" Split into {len(split_docs)} document chunks.\")\n",
    "\n",
    "    #   Step 3: Initialize the Gemini Embedding Model\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "    #   Step 4: Store Documents in ChromaDB Vector Store\n",
    "    vectorstore = Chroma.from_documents(documents=split_docs, embedding=embeddings)\n",
    "\n",
    "    #   Step 5: Create a Retriever for Similarity Search\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "    #   Step 6: Initialize Gemini LLM for Answer Generation\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.3, max_tokens=500)\n",
    "\n",
    "    #   Step 7: Define the System Prompt\n",
    "    system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following retrieved context to answer the question. \"\n",
    "        \"If you don't know the answer, say you don't know. \"\n",
    "        \"Use three sentences maximum and keep the answer concise.\"\n",
    "        \"\\n\\n{context}\"\n",
    "    )\n",
    "\n",
    "    #   Step 8: Create the Prompt Template\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "\n",
    "    #   Step 9: Create Retrieval & Response Chains\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Answer: The brain receives sensory input, processes it in specialized areas, and makes decisions.  These decisions are then relayed to motor areas controlling voluntary muscles.  Involuntary actions are controlled by the mid-brain and hind-brain.\n",
      "üëã Exiting... Goodbye!\n"
     ]
    }
   ],
   "source": [
    "#   Step 10: Get Dynamic User Input and Generate Answer\n",
    "while True:\n",
    "    user_query = input(\"\\n‚ùì Enter your question (or type 'exit' to quit): \")\n",
    "    if user_query.lower() == \"exit\":\n",
    "        print(\"üëã Exiting... Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Run the query through the RAG pipeline\n",
    "    response = rag_chain.invoke({\"input\": user_query})\n",
    "\n",
    "    # Display the result\n",
    "    print(\"\\nüí° Answer:\", response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Answer: The brain has three major parts: the fore-brain, mid-brain, and hind-brain. The fore-brain is responsible for thinking and processing sensory information.  The mid-brain and hind-brain control many involuntary actions.\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"\\n Enter your question? \")\n",
    "\n",
    "    # Run the query through the RAG pipeline\n",
    "response = rag_chain.invoke({\"input\": user_query})\n",
    "\n",
    "    # Display the result\n",
    "print(\"\\nüí° Answer:\", response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
